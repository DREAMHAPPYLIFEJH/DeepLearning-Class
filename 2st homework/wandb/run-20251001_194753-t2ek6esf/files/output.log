Namespace(wandb=False, batch_size=128, epochs=3000)
{'epochs': 3000, 'batch_size': 128, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}
c:\Users\as990\link_dl\_03_homeworks\homework_2\titanic_dataset.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["alone"].fillna(0, inplace=True)
c:\Users\as990\link_dl\_03_homeworks\homework_2\titanic_dataset.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["Embarked"].fillna("missing", inplace=True)
Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',
       'Embarked', 'title', 'family_num', 'alone'],
      dtype='object')
   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \
0       0.0       3    1  22.0      1      0   7.2500         2      2
1       1.0       1    0  38.0      1      0  71.2833         0      3
2       1.0       3    0  26.0      0      0   7.9250         2      1
3       1.0       1    0  35.0      1      0  53.1000         2      3
4       0.0       3    1  35.0      0      0   8.0500         2      2
5       0.0       3    1  29.0      0      0   8.4583         1      2
6       0.0       1    1  54.0      0      0  51.8625         2      2
7       0.0       3    1   2.0      3      1  21.0750         2      0
8       1.0       3    0  27.0      0      2  11.1333         2      3
9       1.0       2    0  14.0      1      0  30.0708         0      3

   family_num  alone
0           1    0.0
1           1    0.0
2           0    1.0
3           1    0.0
4           0    1.0
5           0    1.0
6           0    1.0
7           4    0.0
8           2    0.0
9           1    0.0
Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])
713 178
<torch.utils.data.dataloader.DataLoader object at 0x000001ECF75BBE80>
################################################## 1
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([73])) that is different to the input size (torch.Size([73, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([178])) that is different to the input size (torch.Size([178, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch 100, Training loss 0.2421, Validation loss 0.2351
Epoch 200, Training loss 0.2406, Validation loss 0.2329
Epoch 300, Training loss 0.2428, Validation loss 0.2455
Epoch 400, Training loss 0.2417, Validation loss 0.2323
Epoch 500, Training loss 0.2386, Validation loss 0.2315
Epoch 600, Training loss 0.2394, Validation loss 0.2314
Epoch 700, Training loss 0.2391, Validation loss 0.2311
Epoch 800, Training loss 0.2393, Validation loss 0.2311
Epoch 900, Training loss 0.2389, Validation loss 0.2312
Epoch 1000, Training loss 0.2385, Validation loss 0.2310
Epoch 1100, Training loss 0.2383, Validation loss 0.2312
Epoch 1200, Training loss 0.2412, Validation loss 0.2336
Epoch 1300, Training loss 0.2377, Validation loss 0.2317
Epoch 1400, Training loss 0.2392, Validation loss 0.2312
Epoch 1500, Training loss 0.2383, Validation loss 0.2313
Epoch 1600, Training loss 0.2395, Validation loss 0.2341
Epoch 1700, Training loss 0.2387, Validation loss 0.2324
Epoch 1800, Training loss 0.2382, Validation loss 0.2312
Epoch 1900, Training loss 0.2403, Validation loss 0.2348
Epoch 2000, Training loss 0.2388, Validation loss 0.2322
Epoch 2100, Training loss 0.2377, Validation loss 0.2309
Epoch 2200, Training loss 0.2397, Validation loss 0.2338
Epoch 2300, Training loss 0.2405, Validation loss 0.2333
Epoch 2400, Training loss 0.2391, Validation loss 0.2320
Epoch 2500, Training loss 0.2389, Validation loss 0.2319
Epoch 2600, Training loss 0.2365, Validation loss 0.2315
Epoch 2700, Training loss 0.2391, Validation loss 0.2312
Epoch 2800, Training loss 0.2394, Validation loss 0.2319
Epoch 2900, Training loss 0.2378, Validation loss 0.2309
Epoch 3000, Training loss 0.2400, Validation loss 0.2350
Saved 418 predictions to predictions.csv
