Namespace(wandb=False, batch_size=64, epochs=3000)
{'epochs': 3000, 'batch_size': 64, 'learning_rate': 0.001, 'n_hidden_unit_list': [20, 20]}
c:\Users\as990\link_dl\_03_homeworks\homework_2\titanic_dataset.py:128: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["alone"].fillna(0, inplace=True)
c:\Users\as990\link_dl\_03_homeworks\homework_2\titanic_dataset.py:147: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  all_df["Embarked"].fillna("missing", inplace=True)
Index(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare',
       'Embarked', 'title', 'family_num', 'alone'],
      dtype='object')
   Survived  Pclass  Sex   Age  SibSp  Parch     Fare  Embarked  title  \
0       0.0       3    1  22.0      1      0   7.2500         2      2
1       1.0       1    0  38.0      1      0  71.2833         0      3
2       1.0       3    0  26.0      0      0   7.9250         2      1
3       1.0       1    0  35.0      1      0  53.1000         2      3
4       0.0       3    1  35.0      0      0   8.0500         2      2
5       0.0       3    1  29.0      0      0   8.4583         1      2
6       0.0       1    1  54.0      0      0  51.8625         2      2
7       0.0       3    1   2.0      3      1  21.0750         2      0
8       1.0       3    0  27.0      0      2  11.1333         2      3
9       1.0       2    0  14.0      1      0  30.0708         0      3

   family_num  alone
0           1    0.0
1           1    0.0
2           0    1.0
3           1    0.0
4           0    1.0
5           0    1.0
6           0    1.0
7           4    0.0
8           2    0.0
9           1    0.0
Data Size: 891, Input Shape: torch.Size([891, 10]), Target Shape: torch.Size([891])
713 178
<torch.utils.data.dataloader.DataLoader object at 0x000001ECF76DADA0>
################################################## 1
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
c:\Users\as990\anaconda3\envs\link_dl\lib\site-packages\torch\nn\modules\loss.py:616: UserWarning: Using a target size (torch.Size([178])) that is different to the input size (torch.Size([178, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
Epoch 100, Training loss 0.2453, Validation loss 0.2525
Epoch 200, Training loss 0.2441, Validation loss 0.2460
Epoch 300, Training loss 0.2369, Validation loss 0.2420
Epoch 400, Training loss 0.2394, Validation loss 0.2425
Epoch 500, Training loss 0.2383, Validation loss 0.2412
Epoch 600, Training loss 0.2570, Validation loss 0.2530
Epoch 700, Training loss 0.2346, Validation loss 0.2422
Epoch 800, Training loss 0.2330, Validation loss 0.2468
Epoch 900, Training loss 0.2398, Validation loss 0.2418
Epoch 1000, Training loss 0.2394, Validation loss 0.2411
Epoch 1100, Training loss 0.2419, Validation loss 0.2421
Epoch 1200, Training loss 0.2359, Validation loss 0.2405
Epoch 1300, Training loss 0.2358, Validation loss 0.2408
Epoch 1400, Training loss 0.2394, Validation loss 0.2403
Epoch 1500, Training loss 0.2339, Validation loss 0.2409
Epoch 1600, Training loss 0.2357, Validation loss 0.2406
Epoch 1700, Training loss 0.2400, Validation loss 0.2402
Epoch 1800, Training loss 0.2412, Validation loss 0.2423
Epoch 1900, Training loss 0.2373, Validation loss 0.2405
Epoch 2000, Training loss 0.2352, Validation loss 0.2406
Epoch 2100, Training loss 0.2378, Validation loss 0.2404
Epoch 2200, Training loss 0.2340, Validation loss 0.2428
Epoch 2300, Training loss 0.2352, Validation loss 0.2402
Epoch 2400, Training loss 0.2369, Validation loss 0.2404
Epoch 2500, Training loss 0.2377, Validation loss 0.2401
Epoch 2600, Training loss 0.2373, Validation loss 0.2402
Epoch 2700, Training loss 0.2374, Validation loss 0.2403
Epoch 2800, Training loss 0.2353, Validation loss 0.2409
Epoch 2900, Training loss 0.2388, Validation loss 0.2403
Epoch 3000, Training loss 0.2388, Validation loss 0.2403
Saved 418 predictions to predictions.csv
